---
title: "COREweek13(b)"
author: "Velsamkul"
date: "4/12/2021"
output:
  word_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#Research Question

Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups


## Defining the metric for success
an accuracy above 70%


##Recording the Experimental

1.Data Loading 

2.Data cleaning for missing values and outliers 

3.Exploratory Data Analysis 

4. Perform clustering stating insights drawn from your analysis and visualizations.

5. Upon implementation, provide comparisons between the approaches learned this week i.e. K-Means clustering vs Hierarchical clustering highlighting the strengths and limitations of each approach in the context of your analysis


## Relevance of the Data

The dataset provided has ecommerce data and it is relevant to this analysis.

## Data grossary

'Revenue' attribute can be used as the class label

 "Administrative", "Administrative Duration", "Informational", "Informational Duration", "Product Related" and "Product Related Duration" represents the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real-time when a user takes an action, e.g. moving from one page to another.

  "Bounce Rate", "Exit Rate" and "Page Value" features represent the metrics measured by "Google Analytics" for each page in the e-commerce site.

  "Bounce Rate" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session.
 
  "Exit Rate" feature for a specific web page is calculated as for all pageviews to the page, the percentage that was the last in the session.
  
  "Page Value" feature represents the average value for a web page that a user visited before completing an e-commerce transaction.
  
  "Special Day" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with the transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentine’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.
  
  The dataset also includes the operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.


#Data preparation

##import data packages

```{r}
## 2.0 Importing packages
#
#importing libraries
#
#install.packages("caret")
#loading the library
library(data.table) # load package
#install.packages(c("Rcpp","tidyverse")) # install packages to work with data frame - extends into visualization
library(tidyverse)
library("caret")
```



## Importing dataset
```{r}
# Dataset Url = http://bit.ly/EcommerceCustomersDataset
# Importing our dataset
# ---
#
customer_dataset <- read.csv('http://bit.ly/EcommerceCustomersDataset')
# Previewing the dataset
# ---
#having a look at the dataset 
head(customer_dataset,6)
```

## Previewing the dataset
```{r}
#viewing the dataset
#
View(customer_dataset)
```
observation:  12,330 records and 18 total columns.


## checking the colunms summary
```{r}
# checking the content/summary statistics of each column.
str(customer_dataset)
```
observation: the dataset comprise of integer, numerical, categorical and logical datatypes.

## displaying dataset dimension
```{r}
### checking the dataset dimension(displaying the number of rows and columns).
dim(customer_dataset)
```

```{r}
### checking our dataset class
class(customer_dataset)
```



## converting the data into a tibble for easy manupulation
```{r}
#For ease in analysis,we convert the data into a tibble
df_dataset<-as_tibble(customer_dataset) # there is suggestion to use as_tibble instead of as.tibble.
df_dataset
```


##checking dataset summary
```{r}
### dataset summary 
summary(df_dataset)
```



# Data cleaning



## missing values
```{r}
# Identify missing data in our entire dataset using is.na() function
#
colSums(is.na(df_dataset))
```


```{r}
# using complete function 
df_dataset[!complete.cases(df_dataset),]
```

There are missing values 


```{r}
#dropping missing values
df_dataset <- na.omit(df_dataset)
#
#comfirming no more missing values
colSums(is.na(df_dataset))
```
all missing values were dropped.


```{r}
#checking our dataset dimension after dropping missing values
dim(df_dataset)
```


## Duplicated Data
```{r}
#Identifying Duplicated Data
#
duplicated_rows <- df_dataset[duplicated(df_dataset),]
duplicated_rows
```

```{r}
# checking for duplicated records
anyDuplicated(df_dataset)
```
There are 159 duplicates



```{r}
# removing duplicates using unique () function
df_dataset <- unique(df_dataset)
#
#confirming no duplicates.
anyDuplicated(df_dataset)
```

```{r}
#checking dataset dimension
dim(df_dataset)
```
observation: 159 duplicated records was removed to reduce redundancy. we now have 12199 records and 18 colunms.




##Outliers

```{r}
# installing outlier package
#install.packages("outliers")
#
library(outliers)
```



###converting some columns to categorical

'month', 'The OperatingSystems', 'Browser', 'Region', and 'TrafficType variables', vistor type' are categorical in nature and not numerical, they have already been encoded to make them easier to work with. also 'Weekend' and 'Revenue' columns. We will convert all of them to their appropriate data type.

```{r}
#converting colunms to categorical datatypes
#
df_dataset$Month  <- as.factor(df_dataset$Month)
df_dataset$OperatingSystems <- as.factor(df_dataset$OperatingSystems)
df_dataset$Browser <- as.factor(df_dataset$Browser)
df_dataset$Region <- as.factor(df_dataset$Region)
df_dataset$TrafficType <- as.factor(df_dataset$TrafficType)
df_dataset$VisitorType  <- as.factor(df_dataset$VisitorType)
df_dataset$Weekend <- as.factor(df_dataset$Weekend)
df_dataset$Revenue <- as.factor(df_dataset$Revenue)
```



```{r}
# confirming the data types changes
str(df_dataset)
```


```{r}
#
#Checking the data types of the columns
#
df_num<- df_dataset %>% select_if(is.numeric)
df_num
```
 10 numerical colunms.



```{r}
#checking outliers in numerical colunms
#
outlier(df_num)
```
presence of outliers


```{r}
# getting the summary
summary(df_dataset)
```

On Visitor Type:

10425 are Returning
1693 are New
81 are Other


On Weekend:

9343 are False
2856 are True


On Revenue:

10291 are False
1908 are True



##getting the dataset description

```{r}
library(psych)
describe(df_dataset)
```
observation: it gives us the gimplse of the mean,variance,max and so on.


#Univariate Analysis



## Numerical Data

```{r}
#preview of the numerical colunms
#
df_num <- df_dataset[,1:10]
head(df_num)
```

```{r}
str(df_num)
```



### using boxplots
```{r}
#3.2bi. Checking the outliers in the 'Administrative' Column.
#
boxplot(df_dataset$Administrative, main= "Administrative boxplot",ylab="Administrative", boxwex=0.2)
```
```{r}
#3.2bi. Checking the outliers in the 'Administrative_Duration' Column.
#
boxplot(df_dataset$Administrative_Duration, main= "Administrative_Duration",ylab="Administrative_Duration", boxwex=0.2)
```
```{r}
#3.2bi. Checking the outliers in the 'Informational' Column.
#
boxplot(df_dataset$Informational, main= "Informational",ylab="Informational", boxwex=0.2)
```

```{r}
#3.2bi. Checking the outliers in the 'Informational_Duration' Column.
#
boxplot(df_dataset$Informational_Duration, main= "Informational_Duration",ylab="Informational_Duration", boxwex=0.2)
```


```{r}
#3.2bi. Checking the outliers in the 'ProductRelated' Column.
#
boxplot(df_dataset$ProductRelated, main= "ProductRelated",ylab="ProductRelated", boxwex=0.2)
```

```{r}
#3.2bi. Checking the outliers in the 'ProductRelated_Duration' Column.
#
boxplot(df_dataset$ProductRelated_Duration, main= "ProductRelated_Duration",ylab="ProductRelated_Duration", boxwex=0.2)
```

```{r}
#3.2bi. Checking the outliers in the 'BounceRates' Column.
#
boxplot(df_dataset$BounceRates, main= "BounceRates",ylab="BounceRates", boxwex=0.2)
```

```{r}
#3.2bi. Checking the outliers in the 'ExitRates' Column.
#
boxplot(df_dataset$ExitRates, main= "ExitRates",ylab="ExitRates", boxwex=0.2)
```

```{r}
#3.2bi. Checking the outliers in the 'PageValues' Column.
#
boxplot(df_dataset$PageValues, main= "PageValues",ylab="PageValues", boxwex=0.2)
```

```{r}
#3.2bi. Checking the outliers in the 'SpecialDay' Column.
#
boxplot(df_dataset$SpecialDay, main= "SpecialDay",ylab="SpecialDay", boxwex=0.2)
```
all the numerical columns have outliers but will not be altered because they reflect real wolrd values

### using histograms 

```{r}
#3.3ai installing ggplot2 package for visualization.
#install.packages('ggplot2')
#
#installing ggplot2 library
#
library(ggplot2)
#
#note: the above  have been installed in the console section.
```


```{r}
#visualizing the Administrative colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = Administrative)
```

```{r}
#visualizing the Administrative_Duration colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = Administrative_Duration)
```

```{r}
#visualizing the Informational colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = Informational)
```

```{r}
#visualizing the Informational_Duration colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = Informational_Duration)
```


```{r}
#visualizing the  colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = ProductRelated)
```


```{r}
#visualizing the  ProductRelated_Duration colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = ProductRelated_Duration)
```

```{r}
#visualizing the BounceRates colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = BounceRates)
```


```{r}
#visualizing the ExitRates colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = ExitRates)
```


```{r}
#visualizing the PageValues colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = PageValues)
```

```{r}
#visualizing the SpecialDay  colunm to check for skewness
#
ggplot2::qplot(data = df_dataset, x = SpecialDay)
```
observation: All our numerical variables have outliers are positively skewed.


```{r}
#importing library for visualization
#
library(dplyr)    # alternatively, this also loads %>%
library(tidyr) ## for gather
library("ggplot2") ### for visualization
```


## we can check skewness for numerical values at once as below
```{r}
df_dataset%>%
  gather(attributes, value, 1:10) %>%
  ggplot(aes(x = value)) +
  geom_histogram(fill = 'lightblue2', color = 'black') +
  facet_wrap(~attributes, scales = 'free_x') +
  labs(x="Values", y="Frequency") +
  theme_bw()
```
 positive skewness- to the right


## categorical Data

```{r}
## Identifying the categorical class in the data 
#
#Checking the data types of the columns
#
df_cat<- df_dataset %>% select_if(is.factor)
df_cat
```

###creating tables for bar plots.

```{r}
# create tables of all categorical variables to be able to create bar plots with them
month_table <- table(df_dataset$Month)
os_table <- table(df_dataset$OperatingSystems)
browser_table <- table(df_dataset$Browser)
region_table <- table(df_dataset$Region)
traffic_table <- table(df_dataset$TrafficType)
visitor_table <- table(df_dataset$VisitorType)
weekend_table <- table(df_dataset$Weekend)
revenue_table <- table(df_dataset$Revenue)
```


### adjusting plot size.
```{r}
# function for adjusting plot size
set_plot_dimensions <- function(width_choice, height_choice) {
    options(repr.plot.width = width_choice, repr.plot.height = height_choice)
}
```

```{r}
# barplot of Month
set_plot_dimensions(5, 4)
month_table
barplot(month_table, ylab = "count", xlab="month", col="pink")
```
observation: May is the most frequently occurring month with February being the least frequently occurring.


```{r}
# barplot of operating system
set_plot_dimensions(5, 4)
os_table
barplot(os_table, ylab = "count", xlab="operating system type", col="sky blue")
```
observartion; Operating System 2 is the most widely used Operating System, followed by OS 1 and 3 which seem to be almost the same. OS 5 is the least used operating system.


```{r}
# barplot of browser
set_plot_dimensions(5, 4)
browser_table
barplot(browser_table, ylab = "count", xlab="browser type", col="dark green")
```
observation: Browser 2 is the most widely used browser and it's followed by Browser 1. Browsers 9, 11, and 12 appear to be the least used browsers.



```{r}
# barplot of region
set_plot_dimensions(5, 4)
region_table
barplot(region_table, ylab = "count", xlab="region", col="orange")
```
Region 1 is the most occurring region while Region 5 is the least occurring.



```{r}
# barplot of traffic
set_plot_dimensions(5, 4)
traffic_table
barplot(traffic_table, ylab = "count", xlab="traffic type", col="black")
```
 Traffic Type 2 is the highest followed by Type 1 and Type 3. Type 12, 16, and 17 appear to be the least frequently occurring in the dataset.


```{r}
# barplot of visitor
set_plot_dimensions(5, 4)
visitor_table
barplot(visitor_table, ylab = "count", xlab="visitor type", col="dark orange")
```
Majority of the visitors are returning; they are not new to this business.


```{r}
# barplot of weekend
set_plot_dimensions(5, 4)
weekend_table
barplot(weekend_table, ylab = "count", xlab="day type", col="tomato")
```
Weekdays outnumber weekends in this dataset.


```{r}
# barplot of revenue
set_plot_dimensions(5, 4)
revenue_table 
barplot(revenue_table,ylab = "count", xlab="revenue", col="brown" )
```
There are more FALSE revenues than true 


#Bivariate Analysis

```{r}
# Administrative by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = Administrative, fill = Revenue, color = Revenue)) +
geom_freqpoly(binwidth = 1) +
labs(title = "Administrative by Revenue")
```
false revenue increases sharply upto the pick and eventually drops sharply upto about 100 where administrative cost is 2 and then start to drop gradually. true revenues increases slightly at first and then remains stagnant even as administrative cost increases. 


```{r}
# Administrative Duration by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = Administrative_Duration, fill = Revenue, color = Revenue)) +
geom_histogram(binwidth = 1) +
labs(title = "Administrative Duration by Revenue")
```
True revenue remains the same irrespective of administrative duration.


```{r}
# Informational by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = Informational, fill = Revenue, color = Revenue)) +
geom_freqpoly(binwidth = 1) +
labs(title = "Informational by Revenu")
```
True revenues increases slightly and eventually drops to zero, it remains zero irrespective of an increase in information.



```{r}
# Informational Duration by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = Informational_Duration, fill = Revenue, color = Revenue)) +
geom_histogram(binwidth = 1) +
labs(title = "Informational Duration by Revenue")
```
True revenue remains the same irrespective of information duration.

```{r}
# product related by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = ProductRelated, fill = Revenue, color = Revenue)) +
geom_histogram(binwidth = 1) +
labs(title = "Product Related by Revenue")
```
true revenue remains low despite the product related increasing.



```{r}
# product related duration by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = ProductRelated_Duration, fill = Revenue, color = Revenue)) +
geom_histogram(binwidth = 1) +
labs(title = "Product Related Duration by Revenue")
```
true revenue remains low despite the product related duration increasing.



```{r}
# bounce rates by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = BounceRates, fill = Revenue, color = Revenue)) +
geom_freqpoly(binwidth = 1) +
labs(title = "Bounce Rates by Revenue")
```
bounce rates increases false revenue sharply fron negative to 0 then drop sharply to 1. true revenue also does the same but at lower rate compared to false positive.  

```{r}
# exit rates by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = ExitRates, fill = Revenue, color = Revenue)) +
geom_freqpoly(binwidth = 1) +
labs(title = "Exit Rates by Revenue")
```
exit rates increases false revenue sharply from negative to 0 then drop sharply to 1. true revenue also does the same but at lower rate compared to false positive. 


```{r}
# Page Values by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = PageValues, fill = Revenue, color = Revenue)) +
geom_histogram(binwidth = 1) +
labs(title = "Page Values by Revenue")
```
true revenue remains low despite the page values increasing.

```{r}
# special day by Revenue
set_plot_dimensions(5, 4)
ggplot(df_dataset, aes(x = SpecialDay, fill = Revenue, color = Revenue)) +
geom_freqpoly(binwidth = 1) +
labs(title = "Special Day by Revenue")
```
exit rates increases false revenue sharply from negative to 0 then drop sharply to 1 then they start to reduce gradually. true revenue also does the same but at lower rate compared to false positive.




```{r}
# plotting the distribution of Revenue per Month
set_plot_dimensions(6, 6)
rev_month <- table(df_dataset$Revenue, df_dataset$Month)
barplot(rev_month, main = "Revenue per Month", col = c("pink", "brown"), beside = TRUE, 
legend = rownames(rev_month), ylab="revenue", xlab = "Month")
```
 November returns the highest number of revenues while February returns the lowest.

```{r}
# plotting the distribution of Revenue per Operating System
set_plot_dimensions(6, 6)
rev_os <- table(df_dataset$Revenue, df_dataset$OperatingSystems)
barplot(rev_os, main = "Revenue per Operating System", col = c("sky blue", "brown"), beside = TRUE, 
        legend = rownames(rev_os),ylab="revenue", xlab = "Operating System")
```
Operating System 2 returns the highest number of revenue while OS 5, 6, and 7 return the lowest.

```{r}
# plotting the distribution of Revenue per Browser
set_plot_dimensions(6, 6)
rev_browser <- table(df_dataset$Revenue, df_dataset$Browser)
barplot(rev_browser, main = "Revenue per Browser", col = c("dark green", "brown"), beside = TRUE, 
        legend = rownames(rev_browser),ylab="revenue", xlab = "Browser")
```
 browser 2 returns the highest number of revenue while 3, 7, 9, 11, and 12 return the lowest.


```{r}
# plotting the distribution of Revenue per Region
set_plot_dimensions(6, 6)
rev_region <- table(df_dataset$Revenue, df_dataset$Region)
barplot(rev_region, main = "Revenue per Region", col = c("orange", "brown"), beside = TRUE, 
        legend = rownames(rev_region), ylab="revenue", xlab = "Region")
```
region 1 returns the highest number of revenue, Region 5 and 8 returns the lowest.


```{r}
# plotting the distribution of Revenue per Traffic Type
set_plot_dimensions(6,6)
rev_traffic <- table(df_dataset$Revenue, df_dataset$TrafficType)
barplot(rev_traffic, main = "Revenue per Traffic Type", col = c("black", "brown"), beside = TRUE, 
        legend = rownames(rev_traffic), ylab="revenue", xlab = "Traffic Type")
```
 Traffic 2 has the highest number of revenues, 12, 14, 16, 17, and 18 return the lowest.


```{r}
# plotting the distribution of Revenue per Visitor Type
set_plot_dimensions(6, 6)
rev_visitor <- table(df_dataset$Revenue, df_dataset$VisitorType)
barplot(rev_visitor, main = "Revenue per Visitor Type", col = c("dark orange", "brown"), beside = TRUE, 
        legend = rownames(rev_visitor), ylab="revenue", xlab = "Visitor Type")
```
returning visitors generated more revenue.

```{r}
# plotting the distribution of Revenue per Weekend
set_plot_dimensions(6, 6)
rev_weekend <- table(df_dataset$Revenue, df_dataset$Weekend)
barplot(rev_weekend, main = "Revenue per Weekend", col = c("tomato", "brown"), beside = TRUE, 
        legend = rownames(rev_weekend),ylab="revenue", xlab = "Weekend")
```
 more revenue was generated during the weekdays than the weekends. This is to be expected since there are way more records of weekdays than of weekends.


## correlation of the numerical variables..

```{r}
#install.packages("corrplot") 
library(corrplot)
#
## Let’s build a correlation matrix to understand the relation between each attributes
corrplot(cor(df_num), type = 'upper', method = 'number', tl.cex = 0.9)
```
observation: There is a strong linear correlation between a couple of variables.

1. BounceRates and ExitRates are highly positive correlation of 0.9. 

2. Administrative and Administrative_Duration have a high positive correlation of 0.6. 

3. Informational and Informational_Duration  high positive correlated of 0.62, 

3. ProductRelated and ProductRated_Duration have high positive correlation of 0.86. 

Therefore, we will have to drop one variable of each of the highly correlated pairs to reduce dimensionality and redundancy.


## fitting a linear equation
```{r}
# Relationship between "BounceRates" and "ExitRates"
#
ggplot(df_num, aes(x = BounceRates, y =ExitRates)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) +
  theme_bw()
```


```{r}
# Relationship between "Administrative" and "Administrative_Duration"
#
ggplot(df_num, aes(x = Administrative, y =Administrative_Duration)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) +
  theme_bw()
```




```{r}
# Relationship between "Informational" and "Informational_Duration"
#
ggplot(df_num, aes(x = Informational, y =Informational_Duration)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) +
  theme_bw()
```

```{r}
# Relationship between "ProductRelated" and "ProductRated_Duration"
#
ggplot(df_num, aes(x = ProductRelated, y = ProductRelated_Duration
)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) +
  theme_bw()
```
there is a high positive correlation between the pairs, Therefore, we will have to drop one variable of each of the highly correlated pairs to reduce dimensionality and redundancy.


## dropping the highly correlated columns
```{r}
# selecting highly correlated columns to be dropped
to_drop <- c("Administrative_Duration", "Informational_Duration", "ProductRelated_Duration", "ExitRates")
#
#dropping the highly correlated colunms
#
df_dataset <- df_dataset[, !names(df_dataset) %in% to_drop]
head(df_dataset)
```



## numerical columns for after dropping highly correlated colunms
```{r}
# getting the numerical columns from the new and revised dataframe
new_numeric <- df_dataset[,1:6]
head(new_numeric)
```

## 
```{r}
## Let’s build a correlation matrix to understand the relation between each attributes
corrplot(cor(new_numeric), type = 'upper', method = 'number', tl.cex = 0.9)
```
observation: As we can see, removing the highly correlated variables has  reduced multicollinearity in our dataset. It also made it easier to proceed with the modelling.


##saving our new dataset
```{r}
# save to csv 
write.csv(df_dataset, "customer_new.csv")
```




# Modeling


##Feature engineering
Unsupervised learning requires data that has no labels. So we will create a new dataset that does not have the "Revenue" column.



### previewing the dataset 
```{r}
#previewing the dataset
head(df_dataset)
```


### removing the target/label variable "Revenue"
```{r}
df_new <- df_dataset[, -14]
df_new.revenue <- df_dataset[, "Revenue"]
head(df_new)
```
observation;the dataset to be used contains 13 colunms, this is after dropping the target variable.



```{r}
# Previewing the revenue column
# ---
# 
head(df_new.revenue)
```


## converting factors to numerical for modeling
```{r}
# convert the factors into numerics
df_new$Month <- as.numeric(df_new$Month)
df_new$OperatingSystems <- as.numeric(df_new$OperatingSystems)
df_new$Browser <- as.numeric(df_new$Browser)
df_new$Region <- as.numeric(df_new$Region)
df_new$TrafficType <- as.numeric(df_new$TrafficType)
df_new$VisitorType <- as.numeric(df_new$VisitorType)
df_new$Weekend <- as.numeric(df_new$Weekend)
```

```{r}
## checking the new datatypes
#
str(df_new)
```


```{r}
# checking for missing values
anyNA(df_new)
```



```{r}
# dataset summary
#
summary(df_new)
```


#K-means clustering

let’s prepare our data to do the K means clustering

there are variables which are on a different scale, we need to either scale the data or normalise it. We can normalise the data using the mean and standard deviation, also we can use scale function to normalise our data.


```{r}
# normalizing our dataset by use of scale function.
#
library(dplyr)
df_Norm <- as.data.frame(scale(df_new))
#
#previewing the scaled dataset.
head(df_Norm)
```
our data is now scaled 


#** Computing k-means clustering in R **





```{r}
set.seed(123)
df.new_K7 <- kmeans(df_Norm, centers = 7, nstart = 20)
print(df.new_K7)
```




# for visualization
library("factoextra")
library("cluster")
```


```{r}
#visualize the cluster  so far
#
fviz_cluster(df.new_K7, data = df_Norm)
```


## checking the records in clusters
```{r}
# # previewing the number of records in each cluster
df.new_K7$cluster
```

### checking clusters
```{r}
# previewing the cluster centers 
df.new_K7$centers
```
 these are the 7 clusters data point distribution.

```{r}
# Cluster size
df.new_K7$size
```
 cluster 1 has 931 data point, cluster2 675 datapoints, cluster3 4299,cluster4 1672, cluster5 1872, cluster6 1129, and clsuter7 has 1622 data points.


```{r}
# Between clusters sum of square
df.new_K7$betweenss
```


```{r}
# Within cluster sum of square
df.new_K7$withinss
```


```{r}
# Total with sum of square
df.new_K7$tot.withinss
```


```{r}
# Total sum of square
df.new_K7$totss
```
this is the total sum of all squares.


Because the number of clusters (k) must be set before we start the algorithm, let us use several different values of k and examine the differences in the results.



### We can execute the same process for 2, 3, 4, 5 and 6 clusters, and the results are shown in the figure:
```{r}
df.new_K2 <- kmeans(df_Norm, centers = 2, nstart = 20)
df.new_K3 <- kmeans(df_Norm, centers = 3, nstart = 20)
df.new_K4 <- kmeans(df_Norm, centers = 4, nstart = 20)
df.new_K5 <- kmeans(df_Norm, centers = 5, nstart = 20)
df.new_K6 <- kmeans(df_Norm, centers = 6, nstart = 20)
```

### visualizing
```{r}
library("factoextra")
library("cluster")
library(gridExtra) # for grid.arrange
#We can plot these clusters for different K value to compare.
p1 <- fviz_cluster(df.new_K2, geom = "point", data = df_Norm) + ggtitle(" K = 2")
p2 <- fviz_cluster(df.new_K3, geom = "point", data = df_Norm) + ggtitle(" K = 3")
p3 <- fviz_cluster(df.new_K4, geom = "point", data = df_Norm) + ggtitle(" K = 4")
p4 <- fviz_cluster(df.new_K5, geom = "point", data = df_Norm) + ggtitle(" K = 5")
p5 <- fviz_cluster(df.new_K6, geom = "point", data = df_Norm) + ggtitle(" K = 6")
grid.arrange(p1, p2, p3, p4,p5, nrow = 2)
```

### Determining the optimal number of cluster

Determining Optimal Clusters:

Elbow method
Silhouette method


### elbow method
```{r}
# Determining Optimal clusters (k) Using Elbow method
fviz_nbclust(x = df_Norm,FUNcluster = kmeans, method = 'wss' )
```
observation: elbow methods gives 3 as the optimal cluster, we will check other methods.


### silhouette method
```{r}
# Determining Optimal clusters (k) Using Average Silhouette Method
fviz_nbclust(x = df_Norm,FUNcluster = kmeans, method = 'silhouette' )
```
observation; silhouette method chooses 2 as optimal cluster.




```{r}
library("factoextra")
library("cluster")
library(caret)
library(gridExtra) # for grid.arrange

elbow method suggesting k=3 as optimal cluster and silhouette method giving 2 as optimal cluster.

I choose elbow method suggestion.

```{r}
# Compute k-means clustering with k = 3, optimal cluster
set.seed(123)
final <- kmeans(df_Norm, centers = 3, nstart = 25)
print(final)
```
We can visualize the results using the below code.
```{r}
fviz_cluster(final, data = df_Norm)
```
these are the 3 optimal clusters.

```{r}
# showing how the clusters respond to the revenue variable
table(final$cluster, df_dataset$Revenue)
```
this is how the 3 clusters responds to the revenue variable that was dropped.

##extract the clusters and add to our initial data to do some descriptive statistics at the cluster level
```{r}
df_Norm %>% 
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarize_all('median')
```

summary:
Find optimal number of clusters to be 3 using Elbow method, Silhouette method and Gap-Static method.
and Partited the data using the optimal number of clustering to get the final result.

#Hierarchical clustering



```{r}
# preparation
# ---
# 
desc_stats <- data.frame(
  Min = apply(df_Norm, 2, min),    # minimum
  Med = apply(df_Norm, 2, median), # median
  Mean = apply(df_Norm, 2, mean),  # mean
  SD = apply(df_Norm, 2, sd),      # Standard deviation
  Max = apply(df_Norm, 2, max)     # Maximum
)
desc_stats <- round(desc_stats, 1)
head(desc_stats)
```
the variable has low mean and variance since they are already standardized.


##Agglomerative Nesting (Hierarchical Clustering) using euclidean

we will use the rescaled dataset df_Norm for hierarchical clustering.

```{r}
library(cluster)
# First we use the dist() function to compute the Euclidean distance between observations, 
# res.hc will be the first argument in the hclust() function dissimilarity matrix
# ---
# first we compute the euclidean distance using euclidean metric
eucl_dist<- dist(df_Norm, method = "euclidean")
```

```{r}
# checking euclidean distance in matrix form 
as.matrix(eucl_dist)[1:6, 1:6]
```
this is the euclidean distance of our dataset.

```{r}
# then we compute hierarchical clustering using the Ward method
res_hcl <- hclust(eucl_dist, method =  "ward.D2")
res_hcl
```


## plot the dendogram
```{r}
# Lastly, we plot the obtained dendrogram
# ---
# 
plot(res_hcl, cex = 0.6, hang = -1)
```


##Cut the dendrogram into different groups
This method does not tell us how many clusters there are, or where to cut the dendrogram to form clusters.

You can cut the hierarchical tree at a given height in order to partition your data into clusters.

```{r}
# Cut tree into 60 groups
grp <- cutree(res_hcl, k = 10)
head(grp, n = 10)
```


```{r}
# Number of data points in each cluster
table(grp)
```
this is the number of datapoints in each of the 10 clusters.


## Hierarchical Clustering using manhattan method

```{r}
# We now use the R function hclust() for hierarchical clustering
# ---
# 
# First we use the dist() function to compute the Euclidean distance between observations, 
# d will be the first argument in the hclust() function dissimilarity matrix
# ---
# first we compute the manhattan distance using manhattan metric
#
man_distance <-dist(df_Norm,method= "manhattan")
```

```{r}
#
as.matrix(man_distance)[1:6, 1:6]
```
```{r}
# We then hierarchical clustering using the Ward's method
# ---
# 
man_hc <- hclust(man_distance)
man_hc
```
```{r}
# Lastly, we plot the obtained dendrogram
# ---
# 
plot(man_hc, cex = 0.6, hang = -1)
```

```{r}
### Enhanced Visualization of Dendrogram
# Cut the tree
library(factoextra)
library("ggplot2")
##fviz_dend(man_hc, cex = 0.5, k = 10,color_labels_by_k = TRUE)
```
note: taking long to load, thus i could not persue further. however, we have insights below.

```{r}
# Cut tree into 25 groups
grp <- cutree(man_hc, k = 10)
head(grp, n = 10)
```

```{r}
# Number of data points in each cluster
table(grp)
```
this is the number of data points when we use manhattan 


#conclusion: 
the methods of Agglomerate  hierarchy yields difference number of datapoints in each clusters.
this is because euclidean uses squared errors where as manhattan looks at absolute values.

its better to use k means because it is less computationally intensive.






